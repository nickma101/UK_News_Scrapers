'minority_member', 'issue_importance',
'issue_pro_1', 'issue_con_1',
'group_pro_1', 'group_pro_2', 'group_pro_3', 'group_pro_4',
'group_con_1', 'group_con_2', 'group_con_3', 'group_con_4',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'),
as.numeric)
# remove NA's
df_reads = df_reads %>%
filter(!if_any(c('previous_scroll_rate', 'max_scroll',
'survey_time',
'age',
'gender',
'education',
'modality',
'political_interest',
'political_efficacy_1', 'political_efficacy_2',
'political_efficacy_3', 'political_efficacy_4',
'political_efficacy_5', 'political_Ideology_1',
'last_vote',
'attention',
'transphobia_1', 'transphobia_2', 'transphobia_3',
'transphobia_4', 'transphobia_5', 'transphobia_6',
'transphobia_7', 'transphobia_8',
'minority_member',
'issue_importance',
'issue_pro_1', 'issue_con_1',
'group_pro_1', 'group_pro_2', 'group_pro_3', 'group_pro_4',
'group_con_1', 'group_con_2', 'group_con_3', 'group_con_4',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'), is.na))
knitr::opts_chunk$set(echo = TRUE)
library(Rmisc)
library(tidyverse)
library(ggplot2)
#q = read_csv('qualtrics.csv') %>% select(random_id, EndDate, age)
views = read.csv('views.csv') %>%
dplyr::rename(article_id_new = X_sa_instance_state,
exposure_id_new = article_id,
primary_new = exposure_id,
timestamp_views_new = primary,
user_id_new = timestamp_views)
qualtrics = read.csv("qualtrics.csv") %>%
dplyr::rename(old_user_id = user_id,
user_id = random_id,
survey_time = Duration..in.seconds.) %>%
filter(age > 16) %>%
filter(nchar(user_id) >= 10)
users = read_csv("users.csv") %>% filter(nchar(user_id) >= 10)
positions = read_csv("positions.csv")
all_reads = read_csv("reads.csv")
# check for duplicates
duplicated_reads = duplicated(all_reads$exposure_id) & duplicated(all_reads$article_id) & duplicated(all_reads$primary)
# Remove duplicated rows
reads = all_reads[!duplicated_reads, ]
all_selections = read_csv("selections.csv")
# check for duplicates
duplicated_selections = duplicated(all_selections$exposure_id) & duplicated(all_selections$article_id)
# Remove duplicated rows
selections = all_selections[!duplicated_selections, ]
all_exposures = read_csv("exposures.csv")
# check for duplicates
duplicated_exposures = duplicated(all_exposures$exposure_id)
# Remove duplicated rows
exposures = all_exposures[!duplicated_exposures, ]
# Perform a full outer join to combine selections and consumption data
news_consumption = full_join(selections, reads, by = "user_id")
# Group by user_id and find the closest previous timestamp for each row
news_consumption = news_consumption %>%
group_by(user_id, timestamp_reads) %>%
filter(timestamp_selections == max(timestamp_selections[timestamp_selections < timestamp_reads], na.rm = TRUE)) %>%
ungroup()
# Create final news consumption dataframe incl. all relevant columns
news_consumption = news_consumption %>%
mutate(readingtime = timestamp_reads - timestamp_selections) %>%
select(user_id, article_id.x, article_id.y, previous_scroll_rate,
max_scroll,position, condition,
title, read_title, timestamp_reads, timestamp_selections,
exposure_id.x, exposure_id.y,
) %>%
mutate(readingtime = timestamp_reads-timestamp_selections) %>%
dplyr::rename(aritcle_id = article_id.x,
article_id_reads = article_id.y,
exposure_id = exposure_id.x,
exposure_id_reads = exposure_id.y)
# check for duplicates
duplicated_news_consumption = duplicated(news_consumption$user_id) & duplicated(news_consumption$timestamp_selections)
# Remove duplicated rows
news_consumption = news_consumption[!duplicated_news_consumption, ]
# calculate overall usage time per user
last_exposure = exposures %>%
group_by(user_id) %>%
dplyr::summarise(latest_timestamp = max(timestamp_exposures), number_of_exposures = max(exposure_number))
users = merge(users, last_exposure, by='user_id') %>%
mutate(usage_time = latest_timestamp - timestamp_start)
# df
onlookers <- users %>%
semi_join(exposures, by = "user_id") %>%
anti_join(reads, by = "user_id") %>%
merge(qualtrics, by='user_id')
#merge app data with survey data
df_reads = merge(news_consumption, users, by = 'user_id') %>%
merge(qualtrics, by='user_id')  %>%
select(0:usage_time, survey_time, informed_consent:condition.y) %>%
filter(Code == '123546798123')
# remove NAs
df_all = left_join(exposures, df_reads, by = 'exposure_id')
# keep only one row per user
df = df_reads %>%
arrange(user_id) %>%
filter(duplicated(user_id) == FALSE)
# convert columns to numeric
df_reads = df_reads %>% dplyr::mutate_at(c('previous_scroll_rate', 'max_scroll',
'survey_time',
'age',
'gender',
'education',
'modality',
'political_interest',
'political_efficacy_1', 'political_efficacy_2',
'political_efficacy_3', 'political_efficacy_4',
'political_efficacy_5', 'political_Ideology_1',
'last_vote',
'attention',
'transphobia_1', 'transphobia_2', 'transphobia_3',
'transphobia_4', 'transphobia_5', 'transphobia_6',
'transphobia_7', 'transphobia_8',
'minority_member',
'issue_importance',
'issue_pro_1', 'issue_con_1',
'group_pro_1', 'group_pro_2', 'group_pro_3', 'group_pro_4',
'group_con_1', 'group_con_2', 'group_con_3', 'group_con_4',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'),
as.numeric)
# remove NA's
df_reads %>%
filter(!if_any(c('previous_scroll_rate', 'max_scroll',
'survey_time',
'age',
'gender',
'education',
'modality',
'political_interest',
'political_efficacy_1', 'political_efficacy_2',
'political_efficacy_3', 'political_efficacy_4',
'political_efficacy_5', 'political_Ideology_1',
'last_vote',
'attention',
'transphobia_1', 'transphobia_2', 'transphobia_3',
'transphobia_4', 'transphobia_5', 'transphobia_6',
'transphobia_7', 'transphobia_8',
'minority_member',
'issue_importance',
'issue_pro_1', 'issue_con_1',
'group_pro_1', 'group_pro_2', 'group_pro_3', 'group_pro_4',
'group_con_1', 'group_con_2', 'group_con_3', 'group_con_4',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'), is.na))
# remove NA's
test = df_reads %>%
filter(!if_any(c('survey_time',
'age',
'gender',
'education',
'modality',
'political_interest',
'political_efficacy_1', 'political_efficacy_2',
'political_efficacy_3', 'political_efficacy_4',
'political_efficacy_5', 'political_Ideology_1',
'last_vote',
'attention',
'transphobia_1', 'transphobia_2', 'transphobia_3',
'transphobia_4', 'transphobia_5', 'transphobia_6',
'transphobia_7', 'transphobia_8',
'minority_member',
'issue_importance',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'), is.na))
# remove NA's
df_reads = df_reads %>%
filter(!if_any(c('survey_time',
'age',
'gender',
'education',
'modality',
'political_interest',
'political_efficacy_1', 'political_efficacy_2',
'political_efficacy_3', 'political_efficacy_4',
'political_efficacy_5', 'political_Ideology_1',
'last_vote',
'attention',
'transphobia_1', 'transphobia_2', 'transphobia_3',
'transphobia_4', 'transphobia_5', 'transphobia_6',
'transphobia_7', 'transphobia_8',
'minority_member',
'issue_importance',
'participation_1', 'participation_2', 'participation_3',
'participation_4', 'participation_5', 'participation_6',
'participation_7', 'participation_8', 'participation_9',
'symbolic_threat_1', 'symbolic_threat_2', 'symbolic_threat_3',
'realistic_threat_1', 'realistic_threat_2', 'realistic_threat_3',
'realistic_threat_4', 'realistic_threat_5', 'realistic_threat_6',
'user_satisfaction_1', 'user_satisfaction_2',
'user_satisfaction_3',
'perceived_diversity_1', 'perceived_diversity_2',
'perceived_diversity_3',
'manipulation_check_1', 'manipulation_check_2',
'manipulation_check_3', 'manipulation_check_4'), is.na))
aov(issue_tolerance ~ alternative_voices, data)
H1b = aov(issue_tolerance ~ alternative_voices, data)
summaru(H1b)
summary(H1b)
TukeyHSD(H1b)
H1b = aov(issue_tolerance ~ alternative_voices + issue_position, data)
H1b = aov(issue_tolerance ~ alternative_voices + age + gender + education, data)
summary(H1b)
TukeyHSD(H1b)
H1b = aov(issue_tolerance ~ alternative_voices + age + gender + education, data)
summary(H1b)
TukeyHSD(H1b)
H1b = aov(issue_tolerance ~ alternative_voices + political_interest, data)
summary(H1b)
H1b = aov(issue_tolerance ~ alternative_voices + issue_importance, data)
summary(H1b)
TukeyHSD(H1b)
summary(H1b)
View(data)
json_file_path <- "inews_articles2023-11-09.json"
df_inews <- jsonlite::fromJSON(json_file_path)
library(tidyverse)
#library(rjson)
#library(data.table)
library(jsonlite)
df_inews <- jsonlite::fromJSON(json_file_path)
setwd("~/development/UK_News_Scrapers/production/data")
library(tidyverse)
#library(rjson)
#library(data.table)
library(jsonlite)
json_file_path <- "inews_articles2023-11-09.json"
df_inews <- jsonlite::fromJSON(json_file_path)
View(df_inews)
View(df_inews[[14]][[1]])
library(tidyverse)
#library(rjson)
#library(data.table)
library(jsonlite)
json_file_path <- "bbc_articles2023-11-14.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
json_file_path <- "guardian_articles2023-11-14.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
json_file_path <- "independent_articles2023-11-14.json"
df_independent <- jsonlite::fromJSON(json_file_path)
json_file_path <- "inews_articles2023-11-14.json"
df_inews <- jsonlite::fromJSON(json_file_path)
json_file_path <- "sky_articles2023-11-14.json"
df_sky <- jsonlite::fromJSON(json_file_path)
json_file_path <- "standard_articles2023-11-14.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-05', datePublished) | grepl('20231105', datePublished))
topics = df %>%
group_by(primaryCategory) %>%
summarise(n = n())
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-13', datePublished) | grepl('20231113', datePublished))
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
str(df_bbc)
View(df_bbc)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard)
View(df_standard)
df_bbc$dateScraped = as.character(df_bbc$dateScraped )
df_guardian$dateScraped = as.character(df_guardian$dateScraped )
df_independent$dateScraped = as.character(df_independent$dateScraped )
df_inews$dateScraped = as.character(df_inews$dateScraped )
df_sky$dateScraped = as.character(df_sky$dateScraped )
df_standard$dateScraped = as.character(df_standard$dateScraped )
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-13', datePublished) | grepl('20231113', datePublished))
topics = df %>%
group_by(primaryCategory) %>%
summarise(n = n())
View(topics)
View(df)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
topics = df %>%
group_by(primaryCategory) %>%
summarise(n = n())
View(topics)
library(tidyverse)
#library(rjson)
#library(data.table)
library(jsonlite)
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
df_bbc$dateScraped = as.character(df_bbc$dateScraped )
json_file_path <- "guardian_articles2023-11-15.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
df_guardian$dateScraped = as.character(df_guardian$dateScraped )
json_file_path <- "independent_articles2023-11-15.json"
df_independent <- jsonlite::fromJSON(json_file_path)
df_independent$dateScraped = as.character(df_independent$dateScraped )
json_file_path <- "inews_articles2023-11-15.json"
df_inews <- jsonlite::fromJSON(json_file_path)
df_inews$dateScraped = as.character(df_inews$dateScraped )
json_file_path <- "sky_articles2023-11-15.json"
df_sky <- jsonlite::fromJSON(json_file_path)
df_sky$dateScraped = as.character(df_sky$dateScraped )
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$dateScraped = as.character(df_standard$dateScraped )
#data = fromJSON(file = "guardian_articles2023-11-02.json")
#list_data <- Map(as.data.frame, data)
#df_guardian <- rbindlist(list_data, fill=TRUE)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
library(tidyverse)
#library(rjson)
#library(data.table)
library(jsonlite)
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
df_bbc$dateScraped = as.character(df_bbc$dateScraped )
json_file_path <- "guardian_articles2023-11-15.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
df_guardian$dateScraped = as.character(df_guardian$dateScraped )
json_file_path <- "independent_articles2023-11-15.json"
df_independent <- jsonlite::fromJSON(json_file_path)
df_independent$dateScraped = as.character(df_independent$dateScraped )
json_file_path <- "inews_articles2023-11-15.json"
df_inews <- jsonlite::fromJSON(json_file_path)
df_inews$dateScraped = as.character(df_inews$dateScraped )
json_file_path <- "sky_articles2023-11-15.json"
df_sky <- jsonlite::fromJSON(json_file_path)
df_sky$dateScraped = as.character(df_sky$dateScraped )
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$dateScraped = as.character(df_standard$dateScraped )
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
df_bbc = df_bbc %>% select(_id, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
df_bbc = df_bbc %>% select(_id, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
df_bbc = df_bbc %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
df_bbc$dateScraped = as.character(df_bbc$dateScraped )
df_bbc = df_bbc %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "guardian_articles2023-11-15.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
df_guardian$dateScraped = as.character(df_guardian$dateScraped )
df_guardian = df_guardian %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "independent_articles2023-11-15.json"
df_independent <- jsonlite::fromJSON(json_file_path)
df_independent$dateScraped = as.character(df_independent$dateScraped )
df_independent = df_independent %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "inews_articles2023-11-15.json"
df_inews <- jsonlite::fromJSON(json_file_path)
df_inews$dateScraped = as.character(df_inews$dateScraped )
df_inews = df_inews %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "sky_articles2023-11-15.json"
df_sky <- jsonlite::fromJSON(json_file_path)
df_sky$dateScraped = as.character(df_sky$dateScraped )
df_sky = df_sky %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$dateScraped = as.character(df_standard$dateScraped )
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, dateScraped, outlet, body)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
df_bbc$datePublished = as.character(df_bbc$datePublished )
df_bbc = df_bbc %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "guardian_articles2023-11-15.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
df_guardian$datePublished = as.character(df_guardian$datePublished )
df_guardian = df_guardian %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "independent_articles2023-11-15.json"
df_independent <- jsonlite::fromJSON(json_file_path)
df_independent$datePublished = as.character(df_independent$datePublished )
df_independent = df_independent %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "inews_articles2023-11-15.json"
df_inews <- jsonlite::fromJSON(json_file_path)
df_inews$datePublished = as.character(df_inews$datePublished )
df_inews = df_inews %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "sky_articles2023-11-15.json"
df_sky <- jsonlite::fromJSON(json_file_path)
df_sky$datePublished = as.character(df_sky$datePublished )
df_sky = df_sky %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$datePublished = as.character(df_standard$datePublished )
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, date, outlet, body)
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, date, outlet, body)
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$datePublished = as.character(df_standard$datePublished )
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, date, outlet, body)
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$datePublished = as.character(df_standard$datePublished )
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
json_file_path <- "bbc_articles2023-11-15.json"
df_bbc <- jsonlite::fromJSON(json_file_path)
df_bbc$datePublished = as.character(df_bbc$datePublished )
df_bbc = df_bbc %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "guardian_articles2023-11-15.json"
df_guardian <- jsonlite::fromJSON(json_file_path)
df_guardian$datePublished = as.character(df_guardian$datePublished )
df_guardian = df_guardian %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "independent_articles2023-11-15.json"
df_independent <- jsonlite::fromJSON(json_file_path)
df_independent$datePublished = as.character(df_independent$datePublished )
df_independent = df_independent %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "inews_articles2023-11-15.json"
df_inews <- jsonlite::fromJSON(json_file_path)
df_inews$datePublished = as.character(df_inews$datePublished )
df_inews = df_inews %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "sky_articles2023-11-15.json"
df_sky <- jsonlite::fromJSON(json_file_path)
df_sky$datePublished = as.character(df_sky$datePublished )
df_sky = df_sky %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
json_file_path <- "standard_articles2023-11-15.json"
df_standard <- jsonlite::fromJSON(json_file_path)
df_standard$datePublished = as.character(df_standard$datePublished )
df_standard = df_standard %>% select(`_id`, url, primaryCategory, title, lead, author, datePublished, outlet, body)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE) %>%
filter(grepl('2023-11-14', datePublished) | grepl('20231114', datePublished))
df = rbind(df_bbc, df_guardian, df_independent, df_inews,
df_sky, df_standard, fill=TRUE)
topics = df %>%
group_by(primaryCategory) %>%
summarise(n = n())
View(topics)
